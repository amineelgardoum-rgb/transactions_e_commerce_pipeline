version: "3.8"

services:
  # --- INFRASTRUCTURE SERVICES ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # --- FIX 1: DUAL LISTENER CONFIGURATION ---
      # This tells Kafka how to be reached from inside Docker (INTERNAL) and from your host machine (PLAINTEXT)
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,INTERNAL://kafka:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL # Required for multi-broker setups, good practice for single
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      # --- FIX 3: MORE ROBUST HEALTHCHECK ---
      test: ["CMD", "cub", "kafka-ready", "-b", "kafka:29092", "1", "10"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s

  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    
  kafka-init:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready...' &&
        cub kafka-ready -b kafka:29092 1 30 &&
        echo 'Kafka is ready!' &&
        kafka-topics --create --if-not-exists --topic transactions_topic --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 &&
        echo 'Topic transactions_topic created successfully.'
      "
  
  # --- APPLICATION SERVICES ---
  # Assuming this is your Faker producer
  producer:
    build:
      context: ./producer
    container_name: fastapi-producer
    ports:
      - "8000:8000"
    depends_on:
      # --- FIX 2: CORRECT DEPENDENCY ON A ONE-OFF CONTAINER ---
      - kafka-init
    environment:
      # Point to the internal listener
      KAFKA_BROKER: kafka:29092

  consumer:
    build:
      context: ./consumer
    container_name: python-consumer
    depends_on:
      # --- FIX 2: CORRECT DEPENDENCY ON A ONE-OFF CONTAINER ---
      kafka-init:
         condition: service_completed_successfully
      mongodb:
         condition: service_healthy
    environment:
      KAFKA_BROKER: "kafka:29092"
      MONGO_URI: "mongodb://mongodb:27017"
    
  mockoon:
    build:
      context: ./mockoon
    ports:
      - "3001:3000"
    restart: unless-stopped
  
  producer-mockoon:
    build:
      context: ./producer-mockoon
    ports:
     - "8001:8001"
    depends_on:
     - mockoon
     - kafka-init # This was already correct!
    environment:
     KAFKA_BROKER: kafka:29092
    volumes:
      - ./producer-mockoon:/app

  frontend:
    build:
      context: ./users_transactions_over_time
    container_name: vite-frontend
    ports:
      - "5173:5173"
    # --- FIX 4 (Recommendation): REMOVED UNNECESSARY DEPENDENCY ---
    # depends_on:
    #   - producer
    
# --- VOLUME DEFINITIONS ---
volumes:
  mongo-data:
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
#                                Default: apache/airflow:2.9.2
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''
#
# Feel free to modify this file to suit your needs.